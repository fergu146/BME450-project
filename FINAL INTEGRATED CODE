import torch
from torch import nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import kagglehub
import os
import pandas as pd
import random
import ast

transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize images to 64x64
    transforms.ToTensor()  # Convert images to PyTorch tensors
])

training_data = datasets.ImageFolder(root="Ingredients/Training", transform=transform)
test_data = datasets.ImageFolder(root="Ingredients/Testing", transform=transform)

from PIL import Image
import os

def convert_all_png_to_rgba(input_folder):
    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.lower().endswith(".png"):  # ONLY process PNGs
                path = os.path.join(root, file)
                try:
                    img = Image.open(path)
                    if img.mode == 'P':
                        print(f"Converting {path} from 'P' to 'RGBA'")
                        img = img.convert('RGBA')
                        img.save(path)  # Overwrite in-place
                except Exception as e:
                    print(f"Error converting {path}: {e}")

convert_all_png_to_rgba("Vegetables/Train")
convert_all_png_to_rgba("Vegetables/Test")

train_loader = DataLoader(training_data, batch_size=4184, shuffle=True)
test_loader = DataLoader(test_data, batch_size=587, shuffle=False)


categories = ["Amaranth", "Apple", "Banana", "beans", "Beetroot", "Bell pepper", "Bitter Gourd", 
    "Blueberry", "Bottle Gourd", "Broccoli", "Cabbage", "Cantaloupe", "Capsicum", 
    "Carrot", "Cauliflower", "chicken", "chickpea", "Chilli pepper", "Coconut", "Corn", "Cucumber", 
    "Dragon_fruit", "Eggplant", "eggs", "Fig", "fish", "Garlic", "Ginger", "Grapes", "ground beef", "Jalepeno", 
    "Kiwi", "lamb", "Lemon", "Mango", "Okra", "Onion", "Orange", "Paprika", "Pear", 
    "Peas", "Pineapple", "Pomegranate", "Potato", "Pumpkin", "Raddish", "Raspberry", 
    "Ridge Gourd", "Soy beans", "Spinach", "Spiny Gourd", "Sponge Gourd", 
    "Strawberry", "Sweetcorn", "Sweetpotato", "Tomato", "Turnip", "Watermelon"]


# select a random sample from the training set
sample_num = 2
# print(training_data[sample_num])
print('Inputs sample - image size:', training_data[sample_num][0].shape)
print('Label:', training_data[sample_num][1], '\n')

import matplotlib.pyplot as plt

ima = training_data[sample_num][0]
print('Inputs sample - min,max,mean,std:', ima.min().item(), ima.max().item(), ima.mean().item(), ima.std().item())
ima = (ima - ima.mean())/ ima.std()
ima = torch.clamp(ima, 0, 1)
print('Inputs sample normalized - min,max,mean,std:', ima.min().item(), ima.max().item(), ima.mean().item(), ima.std().item())
iman = ima.permute(1, 2, 0) # needed to be able to plot
plt.imshow(iman)
plt.show()

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.25)
        self.fc1 = nn.Linear(64 * 32 * 32, 512)
        self.fc2 = nn.Linear(512, len(categories))
        
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  # -> (32, 64, 64)
        x = self.pool(F.relu(self.conv2(x)))  # -> (64, 32, 32)
        x = x.view(x.size(0), -1)             # flatten to (batch_size, 65536)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)
        return x
    
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    total_loss = 0
    for batch, (X, y) in enumerate(dataloader):
        # Compute prediction and loss
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
            
    avg_loss = total_loss / len(dataloader)
    return avg_loss


def test_loop(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    test_loss, correct = 0, 0

    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()

    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

#TRAIN THE NETWORK

model = CNN()

batch_size = 4184
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)

loss_fn = nn.CrossEntropyLoss() # used for categorization
learning_rate = 1e-3
# note: optimizer is Adam: one of the best optimizers to date
# it can infer learning rate and all hyper-parameters automatically
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

train_losses = []

epochs = 20
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(train_dataloader, model, loss_fn, optimizer)
    test_loop(test_dataloader, model, loss_fn)
    avg_loss = train_loop(train_dataloader, model, loss_fn, optimizer)
    train_losses.append(avg_loss)
print("Done!")

plt.figure(figsize=(10, 5))
plt.plot(range(1, epochs + 1), train_losses, marker='o')
plt.title("Training Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

# Identify Ingredients based on Image
sample_numbers = [743]  # <- put any sample indices you want here
predicted_classes = []

for sample_num in sample_numbers:
    image = training_data[sample_num][0]

    # Normalize and clamp for visualization (optional)
    ima = (image - image.mean()) / image.std()
    ima = torch.clamp(ima, 0, 1)
    iman = ima.permute(1, 2, 0)

    # Predict
    image = image.unsqueeze(0)  # Add batch dimension
    with torch.no_grad():
        r = model(image)

    class_index = torch.argmax(r).item()
    predicted_class = categories[class_index]

    predicted_classes.append(predicted_class)

print(predicted_classes)

# Download latest version
path = kagglehub.dataset_download("shuyangli94/food-com-recipes-and-user-interactions")
 
print("Path to dataset files:", path)
 
files = os.listdir(path)
print(files)
 
Raw_recipes = os.path.join(path, "RAW_recipes.csv")
print(Raw_recipes)
 
Raw_recipes_file = pd.read_csv(Raw_recipes)

search_words = predicted_classes # Array of ingredients from data set
search_words = [word.lower() for word in predicted_classes]  # Normalize to lowercase
matching_indexes_30 = []
matching_indexes_60 = []

for i, row in Raw_recipes_file.iterrows():
    if all(word in row['ingredients'] for word in search_words):
        tags = ast.literal_eval(row['tags'])
        cooking_time = None

        for tag in tags:
            if tag.endswith("-minutes-or-less"):
                cooking_time = int(tag.split('-')[0])
                break

        if cooking_time and cooking_time <= 30:
            matching_indexes_30.append(i)
        elif cooking_time and cooking_time <= 60:
            matching_indexes_60.append(i)

# Select and print recipes separately for each time category
def print_recipe(matching_indexes, time_category):
    if matching_indexes:
        random_index = random.choice(matching_indexes)
        recipe_name = Raw_recipes_file.name[random_index]
        ingredients_string = Raw_recipes_file.ingredients[random_index]
        ingredients_list = ast.literal_eval(ingredients_string)
        steps_string = Raw_recipes_file.steps[random_index]
        steps_list = ast.literal_eval(steps_string)

        print(f"Chosen ingredient: {search_words}\n")
        print(f"Recipe for {time_category}:\n") # Indicate time category
        print(f"Recipe Name: {recipe_name}\n")
        print(f"Ingredients:\n")
        for ingredient_num, ingredients in enumerate(ingredients_list,1):
            print(f"{ingredient_num}. {ingredients}\n")

        print(f"Steps:\n")
        for step_num, steps in enumerate(steps_list, 1):
            print(f" Step {step_num}: {steps}\n")

        print("\n\n")

    else:
        print(f"No recipes found for {time_category} with '{search_words}'\n")

# Call the function for each time category
print_recipe(matching_indexes_30, "30 minutes or less")
print_recipe(matching_indexes_60, "60 minutes or less")
